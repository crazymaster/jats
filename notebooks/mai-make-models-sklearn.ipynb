{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 毎日新聞データからのモデル作成\n",
    "一般向けと小学生向け記事の対応付け\n",
    "## データクリーニング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "maisho = pd.read_csv('../data/毎日新聞コーパス/maisho2018-utf8.csv')\n",
    "# maisho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mojimoji import zen_to_han \n",
    "maisho.columns = ['ID', 'C0', 'AD', 'AE', 'AF', 'ZZ', 'T1', 'S1', 'S2', 'T2', 'KB']\n",
    "maisho.drop(maisho.index[maisho.ZZ == '著作権無'], inplace=True)\n",
    "maisho.drop(maisho.index[maisho.T1.str.contains('新聞休みます')], inplace=True)\n",
    "maisho = maisho.applymap(lambda x: zen_to_han(str(x), kana=False))\n",
    "maisho.drop_duplicates(subset='T2', inplace=True)\n",
    "# maisho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ふりがなの削除 ()の中身がひらがなのときのみ\n",
    "maisho.T2 = maisho.T2.str.replace('\\([\\u3041-\\u309F]+\\)', '')\n",
    "# 不要な記号の削除\n",
    "maisho.T2 = maisho.T2.str.replace(r'[;◆◇▽●★〓…]', '')\n",
    "# maisho.T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maisho['T2_len'] = maisho.T2.str.len()\n",
    "print(maisho.T2_len.describe())\n",
    "maisho.T2_len.plot.hist(bins=200,range=(50,2000))\n",
    "# maisho.to_csv('../results/maisho_len.csv', index=False)\n",
    "# print(maisho[maisho.T2_len < 100][['T1', 'T2']])\n",
    "maisho.drop(maisho.index[maisho.T2_len < 100], inplace=True)\n",
    "maisho.drop(columns=['T2_len', 'AE', 'ZZ', 'S2'], inplace=True)\n",
    "maisho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mai = pd.read_csv('../data/毎日新聞コーパス/mai2018-utf8.csv')\n",
    "# mai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mai.drop(mai.index[mai.ZZ == '著作権無'], inplace=True)\n",
    "mai.drop(mai.index[mai.T1.str.contains('(?:朝刊|夕刊)休みます')], inplace=True)\n",
    "mai.drop(mai.index[mai.T1.str.contains('東日本大震災・(?:空間|大気中の環境)放射線量')], inplace=True)\n",
    "mai.drop(mai.index[mai.T1.str.contains('当選番号')], inplace=True)\n",
    "mai.drop(mai.index[mai.T1.str.contains(r'ロシアW杯:')], inplace=True)\n",
    "mai.drop(mai.index[mai.T1.str.contains(r'全国高校ラグビー:')], inplace=True)\n",
    "mai.drop(mai.index[mai.T1.str.contains(r'米大リーグ:')], inplace=True)\n",
    "mai.drop(mai.index[mai.T1.str.contains(r'プロ野球:')], inplace=True)\n",
    "mai.drop(mai.index[mai.T1.str.contains(r'東京六大学野球:')], inplace=True)\n",
    "mai.drop(mai.index[mai.T1.str.contains(r'ハンドボール:')], inplace=True)\n",
    "mai.drop(mai.index[mai.T2.str.contains('当選金額')], inplace=True)\n",
    "mai.drop_duplicates(subset='T2', inplace=True)\n",
    "# ふりがなの削除 ()の中身がひらがなのときのみ\n",
    "mai.T2 = mai.T2.str.replace('\\([\\u3041-\\u309F]+\\)', '')\n",
    "# 不要な記号の削除\n",
    "mai.T2 = mai.T2.str.replace(r'[;◆◇●▽★〓…]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mai['T2_len'] = mai.T2.str.len()\n",
    "print(mai.T2_len.describe())\n",
    "mai.T2_len.plot.hist(bins=100,range=(0,200))\n",
    "# mai.to_csv('../results/mai_len.csv', index=False)\n",
    "mai.drop(mai.index[mai.T2_len < 150], inplace=True)\n",
    "mai.drop(columns=['T2_len', 'AA', 'AB', 'AE', 'ZZ', 'S2'], inplace=True)\n",
    "mai"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Doc2Vecによる類似文章算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import MeCab\n",
    "from typing import List\n",
    "# nd_path = sp.check_output('echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"',\n",
    "#                           shell=True).decode().strip('\\n')\n",
    "# m = MeCab.Tagger(\"-Owakati -d \" + nd_path)\n",
    "m = MeCab.Tagger(\"-Owakati\")\n",
    "def wakati(text: str) -> List[str]:\n",
    "    \"\"\"分かち書きにしてリスト返す\"\"\"\n",
    "    return m.parse(text).split(' ')\n",
    "\n",
    "maisho['wakati'] = maisho.T2.map(wakati)\n",
    "mai['wakati'] = mai.T2.map(wakati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from multiprocessing import cpu_count\n",
    "settings = {\n",
    "    \"dbow300d\": {\"size\": 300,\n",
    "                 \"iter\": 20,\n",
    "                 \"window\": 15,\n",
    "                 \"min_count\": 5,\n",
    "                 \"dm\": 0,  # PV-DBOW\n",
    "                 \"dbow_words\": 1,\n",
    "                 \"workers\": cpu_count()},\n",
    "    \"dmpv300d\": {\"size\": 300,\n",
    "                 \"iter\": 20,\n",
    "                 \"window\": 10,\n",
    "                 \"min_count\": 2,\n",
    "                 \"alpha\": 0.05,\n",
    "                 \"dm\": 1,  # PV-DM\n",
    "                 \"sample\": 0,\n",
    "                 \"workers\": cpu_count()}\n",
    "}\n",
    "\n",
    "from gensim.sklearn_api import D2VTransformer\n",
    "import joblib\n",
    "for setting_name, setting in settings.items():\n",
    "    model = D2VTransformer(**setting)\n",
    "    docvecs = model.fit_transform(pd.concat([mai['wakati'], maisho['wakati']]).to_numpy())\n",
    "    joblib.dump(docvecs, f\"../models/mai-doc2vec-{setting_name}.joblib\", compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import joblib\n",
    "docvecs = joblib.load(\"../models/mai-doc2vec-dmpv300d.joblib\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# コサイン類似度行列を求める\n",
    "sim_matrix = cosine_similarity(docvecs[-len(maisho):, :], docvecs[:len(mai), :])\n",
    "# 対戦表\n",
    "combi_table = pd.DataFrame(sim_matrix, index=maisho['ID'], columns=mai['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "top = combi_table.stack().rename_axis(index=['ID_Maisho', 'ID_Mai'])\n",
    "top = top.reset_index().rename(columns={0: 'SIM'})\n",
    "top = top.sort_values('SIM', ascending=False).drop_duplicates('ID_Maisho')\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mai['ID'] = mai['ID'].astype(str)\n",
    "top_table = top.merge(maisho, left_on='ID_Maisho', right_on='ID').drop(columns='ID')\n",
    "top_table = top_table.merge(mai, left_on='ID_Mai', right_on='ID', suffixes=('_Maisho', '_Mai')).drop(columns='ID')\n",
    "top_table.to_csv('../results/mai_doc2vec_sim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('../results/mai_doc2vec_sim.csv')\n",
    "# result[['SIM', 'T2_Maisho', 'T2_Mai']]"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSI(潜在的意味索引)による類似文章算出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "from typing import List\n",
    "class Tokenizer(object):\n",
    "    \"\"\"単語分割と見出し語化(語形の変化を取り除く)を行う\n",
    "    >>> t = Tokenizer()\n",
    "    >>> t('今日の目標は達成した。;●なので、「次の目標は?」と尋ねる。')\n",
    "    ['今日', 'の', '目標', 'は', '達成', 'する', 'た', 'だ', 'ので', '次', 'の', '目標', 'は', 'と', '尋ねる']\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.parser = MeCab.Tagger()\n",
    "    def __call__(self, doc: str) -> List[str]:\n",
    "        morph_list: List[str] = []\n",
    "        for m in self.parser.parse(doc).splitlines():\n",
    "            if m == 'EOS':\n",
    "                continue  # break でも可\n",
    "        \n",
    "            # タブで区切って、表層形と各種情報を得る\n",
    "            surface, features = m.split('\\t')\n",
    "            feature_list = features.split(',')\n",
    "            \n",
    "            # 原形または表層形をストップワードでなければリストに追加する\n",
    "            if not re.search(r'[。、;◆◇●,★〓「」()<>…]', surface):\n",
    "                morph_list.append(feature_list[6] if feature_list[6] != '*' else surface)\n",
    "        \n",
    "        return morph_list\n",
    "\n",
    "import doctest\n",
    "doctest.testmod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "num_topics = 400\n",
    "vectorizer = TfidfVectorizer(tokenizer=Tokenizer(), use_idf=True, smooth_idf=True)\n",
    "svd_model = TruncatedSVD(n_components=num_topics, algorithm='randomized', n_iter=10, random_state=42)\n",
    "lsi_transformer = Pipeline([('tfidf', vectorizer), ('svd', svd_model)])\n",
    "lsi_matrix = lsi_transformer.fit_transform(pd.concat([mai['T2'], maisho['T2']]).to_numpy())\n",
    "joblib.dump(lsi_matrix, f\"../models/mai-lsi_topics{num_topics}.joblib\", compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import joblib\n",
    "# lsi_matrix = joblib.load(\"../models/mai-lsi_topics400.joblib\")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# コサイン類似度行列を求める\n",
    "sim_matrix = cosine_similarity(lsi_matrix[-len(maisho):, :], lsi_matrix[:len(mai), :])\n",
    "# 対戦表\n",
    "combi_table = pd.DataFrame(sim_matrix, index=maisho['ID'], columns=mai['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "top = combi_table.stack().rename_axis(index=['ID_Maisho', 'ID_Mai'])\n",
    "top = top.reset_index().rename(columns={0: 'SIM'})\n",
    "top = top.sort_values('SIM', ascending=False).drop_duplicates('ID_Maisho')\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mai['ID'] = mai['ID'].astype(str)\n",
    "top_table = top.merge(maisho, left_on='ID_Maisho', right_on='ID').drop(columns='ID')\n",
    "top_table = top_table.merge(mai, left_on='ID_Mai', right_on='ID', suffixes=('_Maisho', '_Mai')).drop(columns='ID')\n",
    "top_table.to_csv('../results/mai_lsi_sim.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('../results/mai_lsi_sim.csv')\n",
    "result[['SIM', 'T2_Maisho', 'T2_Mai']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}