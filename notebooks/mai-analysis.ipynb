{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "maisho = pd.read_csv('../data/毎日新聞コーパス/maisho2018-utf8.csv')\n",
    "maisho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mojimoji import zen_to_han \n",
    "maisho.columns = ['ID', 'C0', 'AD', 'AE', 'AF', 'ZZ', 'T1', 'S1', 'S2', 'T2', 'KB']\n",
    "maisho.drop(maisho.index[maisho.ZZ == '著作権無'], inplace=True)\n",
    "maisho.drop(maisho.index[maisho.T1.str.contains('新聞休みます')], inplace=True)\n",
    "maisho = maisho.applymap(lambda x: zen_to_han(str(x), kana=False))\n",
    "maisho.drop_duplicates(subset='T2', inplace=True)\n",
    "maisho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ふりがなの削除 ()の中身がひらがなのときのみ\n",
    "maisho.T2 = maisho.T2.str.replace('\\([\\u3041-\\u309F]+\\)', '')\n",
    "maisho.T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mai = pd.read_csv('../data/毎日新聞コーパス/mai2018-utf8.csv')\n",
    "mai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mai.drop(mai.index[mai.ZZ == '著作権無'], inplace=True)\n",
    "mai.drop(mai.index[mai.T1.str.contains('(?:朝刊|夕刊)休みます')], inplace=True)\n",
    "mai.drop(mai.index[mai.T1.str.contains('東日本大震災・(?:空間|大気中の環境)放射線量')], inplace=True)\n",
    "mai.drop(mai.index[mai.T2.str.len() < 100], inplace=True)\n",
    "mai['T2_len'] = mai.T2.str.len()\n",
    "mai.drop_duplicates(subset='T2', inplace=True)\n",
    "mai.T2 = mai.T2.str.replace('\\([\\u3041-\\u309F]+\\)', '')\n",
    "print(mai.T2_len.describe())\n",
    "mai.to_csv('../results/mai_len.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mai.T2_len.plot.hist(bins=30,range=(0,2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "def wakati(text: str) -> str:\n",
    "    \"\"\"分かち書きにして返す\"\"\"\n",
    "    # nd_path = sp.check_output('echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"',\n",
    "    #                           shell=True).decode().strip('\\n')\n",
    "    # m = MeCab.Tagger(\"-Owakati -d \" + nd_path)\n",
    "    m = MeCab.Tagger(\"-Owakati\")\n",
    "    return m.parse(text)\n",
    "\n",
    "maisho['wakati'] = maisho.T2.map(wakati)\n",
    "mai['wakati'] = mai.T2.map(wakati)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument\n",
    "documents = [TaggedDocument(words = text.split(\" \"), tags = [ID]) \n",
    "             for df in [mai, maisho] for ID, text in df[['ID', 'wakati']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "settings = {\n",
    "    \"dbow300d\": {\"vector_size\": 300,\n",
    "                 \"epochs\": 20,\n",
    "                 \"window\": 15,\n",
    "                 \"min_count\": 5,\n",
    "                 \"dm\": 0,  # PV-DBOW\n",
    "                 \"dbow_words\": 1,\n",
    "                 \"workers\": cpu_count()},\n",
    "    \"dmpv300d\": {\"vector_size\": 300,\n",
    "                 \"epochs\": 20,\n",
    "                 \"window\": 10,\n",
    "                 \"min_count\": 2,\n",
    "                 \"alpha\": 0.05,\n",
    "                 \"dm\": 1,  # PV-DM\n",
    "                 \"sample\": 0,\n",
    "                 \"workers\": cpu_count()}\n",
    "}\n",
    "\n",
    "for setting_name, setting in settings.items():\n",
    "    model = Doc2Vec(documents=documents, **setting)\n",
    "    model.save(f\"../models/mai-doc2vec-{setting_name}.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"../models/mai-doc2vec-dmpv300d.model\")\n",
    "\n",
    "def find_similar(tag):\n",
    "    for t, sim in model.docvecs.most_similar(tag, topn=30):\n",
    "        if 'S' not in str(t):\n",
    "            return str(t), sim\n",
    "    \n",
    "    return np.nan, np.nan\n",
    "\n",
    "maisho['SIM_ID'], maisho['SIM'] = zip(*maisho.ID.map(find_similar))\n",
    "maisho[['SIM_ID','SIM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "maisho.dropna(subset = ['SIM_ID'])\n",
    "maisho[['SIM_ID','SIM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mai['ID'] = mai['ID'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.merge(maisho, mai, left_on='SIM_ID', right_on='ID', suffixes=('_Maisho', '_Mai'))\n",
    "result.to_csv('../results/mai_sim.csv', index=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('../results/mai_sim.csv')\n",
    "result[['SIM', 'T2_Maisho', 'T2_Mai']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import MeCab\n",
    "from gensim import corpora\n",
    "from typing import List\n",
    "parser = MeCab.Tagger()\n",
    "def tokenize(text: str) -> List[str]:\n",
    "    \"\"\"形態素への分割とステミング(語形の変化を取り除く)を行う\n",
    "    >>> tokenize('自分の目標を達成した;●ゴルフの選手')\n",
    "    ['自分', 'の', '目標', 'を', '達成', 'する', 'た', 'ゴルフ', 'の', '選手']\n",
    "    \"\"\"\n",
    "    morph_list: List[str] = []\n",
    "    for m in parser.parse(text).splitlines():\n",
    "        if m == 'EOS':\n",
    "            continue  # break でも可\n",
    "    \n",
    "        # タブで区切って、表層形と各種情報を得る\n",
    "        surface, features = m.split('\\t')\n",
    "        feature_list = features.split(',')\n",
    "        \n",
    "        # 原形または表層形を不要語になければリストに追加する\n",
    "        if not re.search(r'[。、;◆◇●,★〓「」()<>…]', surface):\n",
    "            morph_list.append(feature_list[6] if feature_list[6] != '*' else surface)\n",
    "    \n",
    "    return morph_list\n",
    "\n",
    "mai['Token'] = mai.T2.map(tokenize)\n",
    "maisho['Token'] = maisho.T2.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [text for df in [mai, maisho] for text in df['Token'].values]\n",
    "dic = corpora.Dictionary(documents) \n",
    "dic.filter_extremes(no_below=20, no_above=0.3) \n",
    "bow_corpus = [dic.doc2bow(d) for d in documents] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    " \n",
    "tfidf_model = models.TfidfModel(bow_corpus) \n",
    "tfidf_corpus = tfidf_model[bow_corpus]\n",
    "lsi_model = models.LsiModel(tfidf_corpus, id2word=dic, num_topics=400) \n",
    "lsi_corpus = lsi_model[tfidf_corpus] \n",
    "lsi_model.save('../models/lsi_topics400.model')\n",
    "# https://stackoverflow.com/questions/28488714/retrieve-string-version-of-document-by-id-in-gensim\n",
    "# MmCorpus.serialize(outp + '_bow.mm', corpus, progress_cnt=10000, metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "lsi_model = models.LsiModel.load('../models/lsi_topics400.model')\n",
    "\n",
    "from gensim.test.utils import common_corpus, common_dictionary, get_tmpfile\n",
    "from gensim.similarities import Similarity\n",
    "index_tmpfile = get_tmpfile(\"index\")\n",
    "index = similarities.MatrixSimilarity(lsi_model[lsi_corpus])\n",
    "\n",
    "# https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html#sphx-glr-auto-examples-core-run-similarity-queries-py\n",
    "# https://radimrehurek.com/gensim/similarities/docsim.html\n",
    "# index = Similarity(index_tmpfile, lsi_corpus, num_features=len(dic)) # build the index\n",
    "# the batch is simply an iterable of documents, aka gensim corpus:\n",
    "# for similarities in index:\n",
    "#     pass\n",
    "\n",
    "index.save('../models/lsi.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity.load('../models/lsi.index')\n",
    "\n",
    "result_data = []\n",
    "for i, doc in enumerate(lsi_corpus[len(mai):]):\n",
    "    vec_lsi = lsi_model[doc]\n",
    "    sims = index[vec_lsi]\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "    maisho_txt = ''.join(documents[len(mai) + i])\n",
    "    for idx, sim in sims:\n",
    "        if idx < len(mai):\n",
    "            result_data.append((maisho_txt, i, ''.join(documents[idx]), idx, sim))\n",
    "            break\n",
    "\n",
    "result_df = pd.DataFrame(result_data, columns=['Maisho_Txt','Maisho_Pos', 'Mai_Txt', 'Mai_Pos', 'Sim'])\n",
    "result_df.to_csv('../results/mai_LSI_sim.csv', index=False)\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}